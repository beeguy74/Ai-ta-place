{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beeguy74/Ai-ta-place/blob/main/gradio_stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "AhNeXbMd8J7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86314ccb-c7fa-4553-ba01-b8c1722f2b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import BarkModel, BarkProcessor"
      ],
      "metadata": {
        "id": "RL4yNhGW8M_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX_QBiBs5nQG"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from pydub import AudioSegment\n",
        "from time import sleep\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# synthesiser = pipeline(\"text-to-speech\", \"suno/bark-small\", device=0) #pipeline(\"text-to-speech\")\n",
        "\n",
        "# model = BarkModel.from_pretrained(\"suno/bark-small\")\n",
        "model = BarkModel.from_pretrained(\"suno/bark-small\").to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suQCfIfW9Vz2",
        "outputId": "7e4b5e68-2f83-4661-9133-b94da839264c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BarkProcessor.from_pretrained(\"suno/bark-small\", device=0)\n",
        "# processor = BarkProcessor.from_pretrained(\"suno/bark-small\")"
      ],
      "metadata": {
        "id": "XwgYedrWFp5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from IPython.display import Audio\n",
        "from numpy import squeeze\n",
        "\n",
        "def create_audio(text, lang):\n",
        "  sampling_rate = model.generation_config.sample_rate\n",
        "  voice_preset = \"v2/fr_speaker_1\" if lang == \"french\" else \"v2/en_speaker_3\"\n",
        "  inputs = processor(text, voice_preset=voice_preset)\n",
        "  output = model.generate(**inputs.to(\"cuda\"))\n",
        "  # output = model.generate(**inputs)\n",
        "  return Audio(output.cpu().numpy(), rate=sampling_rate).data"
      ],
      "metadata": {
        "id": "QynoVP-_9jEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            chatbot = gr.Chatbot(\n",
        "                value=[[None, \"Hi! i am friendly cahtbot!\"]],\n",
        "                height=\"50vh\"\n",
        "            )\n",
        "            text = gr.Textbox()\n",
        "\n",
        "        def user(user_message, history):\n",
        "          print(\"USER\", history, file=sys.stderr)\n",
        "          return \"\", history + [[user_message, None]]\n",
        "\n",
        "        def stream_bytes(history):\n",
        "            print(\"STREAM\", history, file=sys.stderr)\n",
        "            chunk_size = 1000\n",
        "            sleep(0.1)\n",
        "            for line in history:\n",
        "              print(\"PROCESSING\", line[0], file=sys.stderr)\n",
        "              res = create_audio(line[0], \"english\")\n",
        "              i = 0\n",
        "              while i < len(res):\n",
        "                  chunk = []\n",
        "                  if i + chunk_size <= len(res):\n",
        "                    chunk = res[i: i + chunk_size]\n",
        "                  else:\n",
        "                    chunk = res[i: ]\n",
        "                  i += chunk_size + 1\n",
        "                  if chunk:\n",
        "                      yield chunk\n",
        "                  else:\n",
        "                      break\n",
        "\n",
        "        with gr.Column():\n",
        "            audio = gr.Audio(\n",
        "                    sources=[\"microphone\"],\n",
        "                    streaming=True,\n",
        "                    label=\"Talk to me\"\n",
        "            )\n",
        "            stream_as_bytes_output = gr.Audio(\n",
        "                autoplay=True,\n",
        "                streaming=True,\n",
        "                label=\"Listen to me\"\n",
        "            )\n",
        "            stream_as_bytes_btn = gr.Button(\"Stream as Bytes\")\n",
        "\n",
        "        text.submit(user, [text, chatbot], [text, chatbot]).then(\n",
        "            stream_bytes, chatbot, stream_as_bytes_output\n",
        "        )\n",
        "        # text.submit(user, [text, chatbot], [text, chatbot])\n",
        "        # stream_as_bytes_btn.click(stream_bytes, chatbot, stream_as_bytes_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "1hJYcdf773ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  demo.queue().launch(debug=True)"
      ],
      "metadata": {
        "id": "5xeaSsCD77K3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c194c104-bf82-4a42-b430-fd6c965fe38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1fb465d82983c3a0f0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1fb465d82983c3a0f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "USER [[None, 'Hi! i am friendly cahtbot!']]\n",
            "STREAM [[None, 'Hi! i am friendly cahtbot!'], ['hi!', None]]\n",
            "PROCESSING None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1352, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 583, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 576, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 559, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 742, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"<ipython-input-11-257ff4abdbb9>\", line 20, in stream_bytes\n",
            "    res = create_audio(line[0], \"english\")\n",
            "  File \"<ipython-input-5-62e74339086e>\", line 6, in create_audio\n",
            "    sampling_rate = model.generation_config.sample_rate\n",
            "NameError: name 'model' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1fb465d82983c3a0f0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "demo.stop()"
      ],
      "metadata": {
        "id": "uC4cPOngj2-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}