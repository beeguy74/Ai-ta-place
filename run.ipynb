{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "302934307671667531413257853548643485645",
      "metadata": {
        "id": "302934307671667531413257853548643485645"
      },
      "source": [
        "# Gradio Demo: stream_asr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272996653310673477252411125948039410165",
      "metadata": {
        "id": "272996653310673477252411125948039410165"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio torch torchaudio transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4WsoUHiF_UF5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WsoUHiF_UF5",
        "outputId": "3ad8742e-6e16-4125-8951-c57ec8cf8175"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x1dx2jWNF7W4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1dx2jWNF7W4",
        "outputId": "db3e8261-ce4b-43a8-a056-d0f1cada754f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \" I don't really have a favorite movie, but I do like action movies. What about you?\"}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot = pipeline(model=\"facebook/blenderbot-400M-distill\", device=0)\n",
        "# Conversation objects initialized with a string will treat it as a user message\n",
        "conversation = chatbot(\"I'm looking for a movie - what's your favourite one?\")\n",
        "conversation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "avvzcrPq_k0t",
      "metadata": {
        "id": "avvzcrPq_k0t"
      },
      "outputs": [],
      "source": [
        "def transcribe(stream, new_chunk):\n",
        "    sr, y = new_chunk\n",
        "    y = y.astype(np.float32)\n",
        "    y /= np.max(np.abs(y))\n",
        "\n",
        "    if stream is not None:\n",
        "        stream = np.concatenate([stream, y])\n",
        "    else:\n",
        "        stream = y\n",
        "    speech_text = transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
        "    response = chatbot(speech_text)\n",
        "    return (\n",
        "        stream,\n",
        "        speech_text,\n",
        "        response[-1][\"generated_text\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288918539441861185822528903084949547379",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "288918539441861185822528903084949547379",
        "outputId": "48490816-360d-48d0-c1e0-6b687139f458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://22720c6b8478f06373.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://22720c6b8478f06373.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://22720c6b8478f06373.gradio.live\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    transcribe,\n",
        "    inputs=[\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n",
        "    outputs=[\"state\", gr.Textbox(label=\"Recognized audio\"), gr.Textbox(label=\"Bot's answer\")],\n",
        "    live=True,\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
